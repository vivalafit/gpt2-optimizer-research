Підхід та реалізація
Агресивне скорочення архітектури:

Ми встановили параметри для student-моделі з GPT‑2, де n_layer = 6, n_embd = 384, n_head = 6.
Ці налаштування значно зменшують кількість параметрів, що підходить для вузькоспеціалізованих завдань.
Прунінг:

Реалізували скрипт, де для лінійних шарів застосовується L1‑unstructured pruning (amount = 0.4).
Модель тренується з використанням Trainer від Hugging Face на CPU (оскільки для квантовання ми працюємо на CPU).
Квантовання:

Після тренування з прунінгом завантажується збережена модель (state_dict).
Якщо в state_dict немає ключів «lm_head.weight_orig» та «lm_head.weight_mask», вони створюються з «lm_head.weight».
Модель явно переводиться на CPU за допомогою model.cpu() перед квантованням.
Налаштовується qconfig з використанням двигуна 'fbgemm', застосовується torch.quantization.prepare() та torch.quantization.convert(), після чого модель зберігається як квантизована.

Основні спостереження
Ефективність дистиляції vs. агресивна оптимізація:
При агресивному скороченні архітектури (n_layer, n_embd, n_head) student-модель може бути занадто "обмеженою" для ефективного засвоєння знань teacher-моделі. Це може призводити до високої perplexity (наприклад, ≈ 2.5), що не є прийнятним для general-purpose LLM, але може бути достатньо для вузьких завдань

Поєднання технік оптимізації:
Ми дослідили можливості поєднання прунінгу та квантовання як пост-тренувальних методів. Дистиляція, з іншого боку, виявилася менш ефективною при такому агресивному скороченні параметрів, оскільки student-модель не має достатньої ємності для передачі всіх тонкощів teacher.

Квантизація на CPU:
Стандартна PyTorch квантовання працює лише на CPU, тому модель потрібно явно перемістити на CPU перед підготовкою та конвертацією. Ми внесли зміни (model.cpu()), щоб уникнути помилок через різні пристрої.

Практичне значення:
Для вузьких завдань (наприклад, генерація вимог до ПЗ) агресивна оптимізація може бути прийнятною, адже модель спеціально донавчається вирішувати конкретну задачу. Проте для general-purpose LLM та універсальних задач компроміс між точністю та стисненням набагато важливіший.

Висновок
Наші дослідження показали, що не всі техніки оптимізації сумісні одна з одною. 
Агресивне обрізання параметрів student-моделі може обмежувати ефективність дистиляції, 
і у такому випадку доцільніше використовувати прунінг та квантовання як пост-тренувальні методи для досягнення високої компактності моделі без значної втрати продуктивності. 
