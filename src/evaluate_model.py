import torch
import os
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from datasets import load_dataset
import math

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å
MODEL_PATH = "./models/fine-tuned/checkpoint-1200"  # –ó–∞–¥–∞—î–º–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π —á–µ–∫–ø–æ–π–Ω—Ç
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Loading model and tokenizer from {MODEL_PATH}...")
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)  # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(device)  # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
test_dataset = load_dataset("csv", data_files={"test": os.path.join(base_dir, "test_iso.csv")})["test"]

# –û—Ü—ñ–Ω–∫–∞ –ø–µ—Ä–ø–ª–µ–∫—Å—ñ—ó
def calculate_perplexity(model, tokenizer, dataset):
    model.eval()
    total_loss = 0
    num_samples = 0

    for sample in dataset:
        input_text = sample["requirement"]
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            total_loss += loss.item()
            num_samples += 1

    avg_loss = total_loss / num_samples
    perplexity = math.exp(avg_loss)  # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Loss —É Perplexity
    return perplexity

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —Ç–µ–∫—Å—Ç—É
def generate_text(prompt, max_length=50):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ—Ü—ñ–Ω–∫—É
print("Evaluating model...")
perplexity = calculate_perplexity(model, tokenizer, test_dataset)
print(f"Model Perplexity: {perplexity:.2f}")

# –¢–µ—Å—Ç—É—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é
print("\nüîπ **Text Generation Examples:**")
test_prompts = [
    "The software must be able to",
    "User authentication must include",
    "The system must provide real-time"
]

for prompt in test_prompts:
    generated_text = generate_text(prompt)
    print(f"\n**Prompt:** {prompt}")
    print(f"**Generated:** {generated_text}")
