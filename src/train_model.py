import os
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# üîπ –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É
def load_data():
    """
    –§—É–Ω–∫—Ü—ñ—è –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞—Ç–∞—Å–µ—Ç–∏ train, val, test —ñ–∑ —Ñ–∞–π–ª—ñ–≤ CSV.
    """
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ—Ä–µ–Ω–µ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é
    print(f"Train file path: {os.path.join(base_dir, 'train_iso.csv')}")
    print(f"Validation file path: {os.path.join(base_dir, 'val_iso.csv')}")
    print(f"Test file path: {os.path.join(base_dir, 'test_iso.csv')}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ñ–∞–π–ª–∏ –≤ —Å–ª–æ–≤–Ω–∏–∫
    dataset = load_dataset(
        "csv", 
        data_files={
            "train": os.path.join(base_dir, "train_iso.csv"),
            "val": os.path.join(base_dir, "val_iso.csv"),
            "test": os.path.join(base_dir, "test_iso.csv"),
        }
    )
    print("Dataset loaded successfully!")
    return dataset

# üîπ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—É
def tokenize_data(dataset, tokenizer):
    """
    –¢–æ–∫–µ–Ω—ñ–∑—É—î –¥–∞–Ω—ñ, –¥–æ–¥–∞—é—á–∏ labels –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É –≤—Ç—Ä–∞—Ç (loss).
    """
    def preprocess_function(examples):
        # –¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ —Ç–µ–∫—Å—Ç —ñ–∑ –ø–∞–¥–¥—ñ–Ω–≥–æ–º, –æ–±—Ä—ñ–∑–∞–Ω–Ω—è–º —ñ max_length
        model_inputs = tokenizer(
            examples["requirement"], 
            truncation=True, 
            padding="max_length", 
            max_length=128
        )
        
        # üîπ –î–æ–¥–∞—î–º–æ labels, —â–æ–± –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ –≤—Ç—Ä–∞—Ç–∏
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        
        return model_inputs
    
    # –¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ –∫–æ–∂–µ–Ω —Å–ø–ª—ñ—Ç (train, val, test)
    tokenized = dataset.map(preprocess_function, batched=True)
    print("Dataset tokenized successfully!")
    return tokenized

# üîπ –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
def train_model():
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ GPT-2.
    """
    # 1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ GPT-2
    model_name = "gpt2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    # üîπ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø–∞–¥–¥—ñ–Ω–≥-—Ç–æ–∫–µ–Ω (GPT-2 –Ω–µ –º–∞—î –π–æ–≥–æ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
    tokenizer.pad_token = tokenizer.eos_token

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —ñ –ø–µ—Ä–µ–º—ñ—â—É—î–º–æ —ó—ó –Ω–∞ GPU (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    print(f"Model {model_name} loaded successfully!")

    # 2Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    dataset = load_data()
    tokenized_datasets = tokenize_data(dataset, tokenizer)

    # 3Ô∏è‚É£ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    training_args = TrainingArguments(
        output_dir="./models/fine-tuned",  # –ö—É–¥–∏ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        evaluation_strategy="epoch",     # –û—Ü—ñ–Ω—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
        save_strategy="epoch",           # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
        num_train_epochs=3,              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
        per_device_train_batch_size=4,   # Batch size –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        per_device_eval_batch_size=4,    # Batch size –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        logging_dir="./logs",            # –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –ª–æ–≥—ñ–≤
        logging_steps=100,               # –Ø–∫ —á–∞—Å—Ç–æ –ª–æ–≥—É–≤–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
        fp16=True,                       # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Mixed Precision Training
        report_to="none",                # –í–∏–º–∫–Ω–µ–Ω–Ω—è WandB –∞–±–æ —ñ–Ω—à–∏—Ö —Ç—Ä–µ–∫–µ—Ä—ñ–≤
    )

    # 4Ô∏è‚É£ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è `Trainer`
    trainer = Trainer(
        model=model,                     # GPT-2 –º–æ–¥–µ–ª—å
        args=training_args,              # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        train_dataset=tokenized_datasets["train"],  # –ù–∞–≤—á–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        eval_dataset=tokenized_datasets["val"],     # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        tokenizer=tokenizer,             # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    )
    print("Trainer initialized successfully!")

    # 5Ô∏è‚É£ –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    print("Training started...")
    trainer.train()
    print("Training completed!")

# üîπ –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
if __name__ == "__main__":
    train_model()
